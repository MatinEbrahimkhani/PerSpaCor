{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 12:29:36.234776: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-07 12:29:36.234797: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
    "                          AutoConfig, Trainer, TrainingArguments, DataCollatorForTokenClassification)\n",
    "\n",
    "# Setting up a proxy for all connections made by the Python interpreter\n",
    "# os.environ['all_proxy'] = \"socks5://127.0.0.1:10808\"\n",
    "\n",
    "# Changing the current working directory to \"/home/matin/Desktop/Masters\"\n",
    "# os.chdir(\"/home/matin/Desktop/Masters\")\n",
    "\n",
    "# Importing custom classes for loading and labeling the corpus\n",
    "from utils.corpusloader import CorpusType\n",
    "from utils.corpusloader import CorpusLoader\n",
    "from utils.labeler import Labeler\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T08:59:37.827443881Z",
     "start_time": "2023-10-07T08:59:34.599583759Z"
    }
   },
   "id": "fd071417052f322a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-07T08:59:41.265007987Z",
     "start_time": "2023-10-07T08:59:37.835621501Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the corpus using the custom CorpusLoader class\n",
    "corpus = CorpusLoader()\n",
    "data = corpus.read_bijan(CorpusType.sents_raw)\n",
    "# TODO: add label 1 to the end of sentances\n",
    "# TODO: add the cls sep to labels\n",
    "# \n",
    "# Labeling the data using the custom Labeler class\n",
    "labeler = Labeler()\n",
    "labeler.set_text(data, corpus_type=CorpusType.sents_raw)\n",
    "chars, labels = labeler.labeler()\n",
    "\n",
    "pretrained_model = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "model_dir = \"./Model1/\"\n",
    "# pretrained_model = \"bert-base-multilingual-cased\"\n",
    "\n",
    "# Limiting the data to the first 30 samples for demonstration purposes\n",
    "chars = chars[:20]\n",
    "labels = labels[:20]\n",
    "\n",
    "# Converting label strings to integers\n",
    "for i, label in enumerate(labels):\n",
    "    labels[i] = [int(l) for l in label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Loading a pre-trained tokenizer and model configuration from Hugging Face's model hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "config = AutoConfig.from_pretrained(pretrained_model)\n",
    "\n",
    "# Modifying the model configuration to have 3 labels for token classification\n",
    "config.num_labels = 3\n",
    "\n",
    "# Initializing a model for token classification with the modified configuration\n",
    "model = AutoModelForTokenClassification.from_config(config)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T08:59:47.511222894Z",
     "start_time": "2023-10-07T08:59:41.269388848Z"
    }
   },
   "id": "38acef6ac5d72f96"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T08:59:47.519460321Z",
     "start_time": "2023-10-07T08:59:47.511358549Z"
    }
   },
   "id": "3d07e4173320b73d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "# Tokenizing the input data and adding special tokens\n",
    "tokens_all = []\n",
    "for char in chars:\n",
    "    tokens = [tokenizer.encode(ch)[1] for ch in char]\n",
    "    tokens.insert(0, 2)  # Adding CLS token at the beginning of each sequence \"2 is the tokenized \"[CLS]\" \"\n",
    "    tokens.append(4)  # Adding SEP token at the end of each sequence \"4 is the tokenized \"[SEP]\" \"\n",
    "    tokens_all.append(tokens)\n",
    "# Truncate the token sequences\n",
    "tokens_all = [tokens[:512] for tokens in tokens_all]\n",
    "\n",
    "# creating the right\n",
    "labels_all = []\n",
    "for label in labels:\n",
    "    lab = label\n",
    "    lab.insert(0,0) # # Adding CLS token label at the beginning of each sequence \n",
    "    lab[-1] = 1 # at the end of a sentence there would be a space obviously\n",
    "    lab.append(0) # Adding SEP token label at the end of each sequence \n",
    "    labels_all.append(lab)\n",
    "# Truncate the labels\n",
    "labels_all = [labels[:512] for labels in labels_all]    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T08:59:47.587568213Z",
     "start_time": "2023-10-07T08:59:47.520576732Z"
    }
   },
   "id": "cd7e3380c60e216a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "# Creating a Dataset object from the tokenized inputs and labels\n",
    "dataset = Dataset.from_dict({\"input_ids\": tokens_all, \"labels\": labels_all})\n",
    "\n",
    "# Initializing a data collator for token classification with the pre-trained tokenizer\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# Setting up training arguments for the Trainer class\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "# Initializing a Trainer instance with the model, training arguments, dataset, tokenizer, and data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T09:02:02.413188179Z",
     "start_time": "2023-10-07T09:02:02.371786376Z"
    }
   },
   "id": "218e25aaaee30ff0"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matin/Desktop/Masters/venv/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/4 : < :, Epoch 0.50/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training the model on the dataset\n",
    "trainer.train()\n",
    "\n",
    "# Saving the trained model in the \"./Model/model/\" directory\n",
    "trainer.save_model(model_dir + \"model/\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T09:02:40.333169619Z",
     "start_time": "2023-10-07T09:02:04.702782070Z"
    }
   },
   "id": "52dec1cc8b6a7afc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2079e61449e7eb27"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
