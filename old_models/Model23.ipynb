{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T13:36:56.251226367Z",
     "start_time": "2023-10-31T13:36:33.621423696Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 17:06:40.786830: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-31 17:06:40.786909: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
    "                          AutoConfig, Trainer, TrainingArguments, DataCollatorForTokenClassification)\n",
    "\n",
    "from transformers import (BertTokenizerFast,\n",
    "                          BertForTokenClassification,\n",
    "                          Trainer,\n",
    "                          TrainingArguments)\n",
    "import torch\n",
    "# Importing custom classes for loading and labeling the corpus\n",
    "from utils.corpusprocessor import CorpusType\n",
    "from utils.corpusprocessor import CorpusLoader\n",
    "from utils.labeler import Labeler\n",
    "\n",
    "# Loading the corpus using the custom CorpusLoader class\n",
    "corpus = CorpusLoader()\n",
    "labeler = Labeler( tags=(1, 2),\n",
    "                 regexes=(r'[^\\S\\r\\n\\v\\f]', r'\\u200c'),\n",
    "                 chars=(\" \", \"â€Œ\"),\n",
    "                 class_count=2,\n",
    "                 )\n",
    "\n",
    "\n",
    "bijan_data = corpus.load_bijan(CorpusType.whole_raw)\n",
    "labeler.set_text(bijan_data, corpus_type=CorpusType.whole_raw)\n",
    "chars, labels = labeler.labeler()\n",
    "chars = chars[:3000]\n",
    "labels = labels[:3000]\n",
    "model_dir = \"./Model2113/\"\n",
    "# pretrained_model = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "pretrained_model = \"bert-base-multilingual-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24b5300-7e97-4fbb-ad68-1baa38181b31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T09:51:55.311352836Z",
     "start_time": "2023-10-26T09:51:52.675568804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model)\n",
    "# Tokenize your data\n",
    "tokenized_inputs = tokenizer(chars, padding=\"max_length\", is_split_into_words=True,)\n",
    "encoded_tokenized = [tokenizer.encode(x)[1] for x in chars]\n",
    "# Prepare the labels. Here I'm simply padding the labels list with -100s (the default ignore index in PyTorch)\n",
    "labels = labels + [-100] * (len(encoded_tokenized) - len(labels))\n",
    "# data = Dataset.from_dict({\"input_ids\": [tokenized_inputs['input_ids']], \"attention_mask\": [tokenized_inputs['attention_mask']], \"labels\": [labels]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa4118836752387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T09:51:55.321247274Z",
     "start_time": "2023-10-26T09:51:55.314636103Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunked_tokenization(tokens, labels,tokenizer,chunk_size=512):\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "\n",
    "   \n",
    "    # Create chunks\n",
    "    for i in range(0, len(tokens), 512 - 2):  # We subtract 2 to account for special tokens\n",
    "        chunk_tokens = tokens[i:i + 512 - 2]\n",
    "        chunk_label_ids = labels[i:i + 512 - 2]\n",
    "\n",
    "        # Add special tokens\n",
    "        chunk_tokens = [tokenizer.encode('[CLS]')[1]] + chunk_tokens + [tokenizer.encode('[SEP]')[1]]\n",
    "        # chunk_tokens = ['[CLS]'] + chunk_tokens + ['[SEP]']\n",
    "        chunk_label_ids = [-100] + chunk_label_ids + [-100]\n",
    "\n",
    "        # Convert tokens to input IDs and create attention mask\n",
    "       \n",
    "        chunk_attention_mask = [1] * len(chunk_tokens)\n",
    "\n",
    "        # Pad sequenceschunk_input_ids = tokenizer.convert_tokens_to_ids(chunk_tokens)\n",
    "        while len(chunk_tokens) < chunk_size:\n",
    "            chunk_tokens.append(0)\n",
    "            chunk_attention_mask.append(0)\n",
    "            chunk_label_ids.append(-100)\n",
    "\n",
    "        input_ids_list.append(chunk_tokens)\n",
    "        attention_mask_list.append(chunk_attention_mask)\n",
    "        labels_list.append(chunk_label_ids)\n",
    "\n",
    "    return input_ids_list, attention_mask_list, labels_list\n",
    "\n",
    "\n",
    "\n",
    "# Use this function to prepare your data\n",
    "input_ids_list, attention_mask_list, labels_list = chunked_tokenization(encoded_tokenized, labels,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719faaaf0bf48d7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Testing the Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "558101e39e8f23f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T09:57:06.021908484Z",
     "start_time": "2023-10-26T09:57:05.996039577Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 512) (6, 512) (6, 512)\n",
      "input_ids_list\n",
      " [[101 108 451 479 475 481 477 463 481 451 461 478 458 451 461 456 451 462\n",
      "  476 477 468 479 476 478 464 476 463 481 459 481 459 478 464 459 119 108\n",
      "  108 479 451 464 477 507 454 477 471 458 452 461 507 462 451 461 481 456\n",
      "  476 478 479 461 481 451 463 475 451 476 481 108 463 454 451 461 478 464\n",
      "  477 451 463 451 477 476 481 507 479 481 477 459 474 478 476 476 474 477\n",
      "  451 463 454 451 479 475 481 477 463 481 451 461 478 458 451 461 456 451\n",
      "  462 476 477 468 479 476 478 464 476 463 481 461 451 459 481 459 478 452\n",
      "  451 464 477 459 119 467 481 463 451 475 478 451 481 451 458 481 461 446\n",
      "  452 481 464 451 462 459 479 459 479 456 481 477 463 481 451 461 478 459\n",
      "  461 476 459 451 461 451 467 461 451 472 463 454 451 461 478 478 451 474\n",
      "  464 472 464 459 478 451 477 459 119 475 481 474 477 463 454 451 461 478\n",
      "  464 477 451 463 451 477 478 461 507 462 451 481 477 463 481 451 461 478\n",
      "  478 451 461 451 479 451 473 469 451 102 477 459 481 459 478 452 479 459\n",
      "  477 459 479 472 473 467 451 462 467 461 481 473 464 479 451 478 459 446\n",
      "  475 461 462 464 478 451 479 451 455 461 451 454 481 474 478 451 477 451\n",
      "  477 452 461 451 456 461 451 476 451 467 461 451 472 458 479 459 476 481\n",
      "  507 460 451 464 454 477 459 446 476 479 472 473 452 478 455 452 454 451\n",
      "  481 477 463 481 451 461 451 454 464 459 478 452 479 459 477 459 119 499\n",
      "  477 481 477 454 465 479 461 476 481 464 459 474 478 451 481 477 475 461\n",
      "  462 464 452 478 469 475 454 456 451 460 452 478 463 481 451 461 478 451\n",
      "  481 459 461 476 459 451 461 452 479 459 478 451 463 454 119 459 461 451\n",
      "  458 461 481 477 454 457 473 481 473 452 478 469 476 475 451 476 459 478\n",
      "  477 481 462 463 454 451 461 478 464 477 451 463 451 477 478 477 479 462\n",
      "  479 451 473 469 451 102 476 479 472 473 452 478 461 479 481 454 476 463\n",
      "  454 473 481 476 463 481 451 461 478 477 464 459 478 451 477 459 446 452\n",
      "  475 474 478 459 461 458 464 464 463 454 451 461 478 451 481 461 451 459\n",
      "  481 459 478 451 477 459 474 478 452 478 451 469 454 473 451 459 451 477\n",
      "  478 451 454 479 463 467 481 474 463 481 451 461 478 476 477 469 474 463\n",
      "  464 459 478 451 463 454 119 102]\n",
      " [101 452 477 451 452 461 451 481 477 507 462 451 461 464 446 459 461 458\n",
      "  464 464 474 479 461 474 477 477 459 478 451 481 477 463 481 451 461 478\n",
      "  446 459 481 459 477 451 477 461 451 477 451 476 457 454 476 475 476 481\n",
      "  474 477 459 119 499 477 451 477 499 478 451 481 477 498 502 479 478 464\n",
      "  507 461 451 477 459 461 463 454 457 459 463 462 459 478 452 451 464 477\n",
      "  459 446 481 451 472 454 478 478 451 481 451 477 478 451 476 481 454 479\n",
      "  451 477 459 452 461 451 481 454 464 458 481 465 457 456 476 479 466 458\n",
      "  451 476 454 459 473 481 473 463 481 451 461 478 476 462 452 479 461 476\n",
      "  479 461 459 451 463 454 472 451 459 478 473 461 451 461 507 481 461 459\n",
      "  119 476 464 461 479 457 451 481 477 507 462 451 461 464 459 461 464 476\n",
      "  451 461 478 451 458 481 461 476 456 475 478 477 481 499 461 452 478 499\n",
      "  451 498 461 463 481 459 478 451 463 454 119 108 459 476 464 473 446 479\n",
      "  451 457 459 476 461 474 462 481 458 452 461 108 476 465 461 472 476 459\n",
      "  451 479 476 476 479 462 476 481 454 479 451 477 459 452 478 476 464 474\n",
      "  475 451 454 459 463 454 507 451 478 507 479 451 461 464 479 452 461 467\n",
      "  461 472 474 461 459 477 477 472 458 476 469 459 478 498 451 481 451 477\n",
      "  459 478 459 119 452 478 507 462 451 461 464 479 451 457 459 476 461 474\n",
      "  462 481 458 452 461 451 462 459 476 464 473 446 461 479 462 477 451 476\n",
      "  478 454 464 461 481 477 477 479 464 454 131 454 457 473 481 473 451 454\n",
      "  498 462 464 474 481 477 464 451 477 459 451 459 478 451 463 454 451 469\n",
      "  454 473 451 459 451 454 464 451 481 469 459 461 452 451 461 478 474 451\n",
      "  475 461 481 462 481 451 459 476 479 456 479 459 459 461 476 481 479 478\n",
      "  476 479 462 479 477 473 464 451 481 477 476 481 479 478 459 461 451 457\n",
      "  463 451 463 463 481 461 481 446 452 481 451 463 451 463 451 463 454 119\n",
      "  451 481 477 461 479 462 477 451 476 478 476 481 477 479 481 463 459 446\n",
      "  478 461 481 474 465 459 507 461 476 476 479 462 122 121 123 474 451 475\n",
      "  461 481 481 469 477 481 123 121 474 451 475 461 481 452 481 464 454 461\n",
      "  451 462 481 474 451 463 454 474 451 477 476 451 463 454 451 477 461 502\n",
      "  481 452 478 452 459 477 476 102]\n",
      " [101 481 459 478 459 479 459 461 469 479 466 476 479 462 476 481 454 479\n",
      "  451 477 459 452 463 481 451 461 481 451 462 476 464 474 475 451 454 507\n",
      "  479 451 461 464 481 461 451 476 451 477 477 459 477 472 458 476 469 459\n",
      "  478 459 461 476 451 477 474 477 459 119 452 481 476 451 461 451 477 476\n",
      "  452 454 475 451 452 478 451 463 476 452 451 481 459 451 462 452 451 475\n",
      "  464 478 451 481 498 461 451 463 454 472 451 459 478 474 477 477 459 119\n",
      "  108 454 478 461 451 477 471 479 451 457 459 476 461 474 462 481 458 452\n",
      "  461 108 451 463 454 472 451 459 478 451 462 452 451 475 464 478 451 481\n",
      "  498 461 452 461 451 481 452 481 476 451 461 451 477 476 452 454 475 451\n",
      "  452 478 451 463 476 446 476 477 451 463 452 454 461 451 462 452 451 475\n",
      "  464 478 451 481 463 451 458 454 478 464 459 478 451 462 451 475 481 451\n",
      "  472 476 465 477 479 469 481 451 463 454 119 452 478 507 462 451 461 464\n",
      "  451 481 477 454 461 477 454 451 462 475 477 459 477 446 498 502 479 478\n",
      "  464 507 461 451 477 481 474 481 451 462 452 481 476 451 461 463 454 451\n",
      "  477 478 451 481 476 477 499 463 454 461 476 481 507 479 481 477 459 131\n",
      "  476 479 451 459 457 463 451 463 481 454 462 451 481 476 479 456 479 459\n",
      "  459 461 451 475 481 451 472 476 465 477 479 469 481 457 459 479 459 128\n",
      "  454 451 129 452 461 451 452 461 476 479 451 459 457 463 451 463 481 454\n",
      "  462 451 481 498 461 451 463 454 119 498 502 479 478 464 507 461 451 477\n",
      "  452 451 452 461 461 463 481 479 476 473 451 481 463 478 122 125 477 479\n",
      "  469 476 458 454 475 472 451 462 452 451 475 464 478 451 481 476 457 454\n",
      "  479 481 451 475 481 451 472 476 465 477 479 469 481 479 498 461 452 478\n",
      "  451 481 477 477 454 481 456 478 461 463 481 459 478 451 477 459 474 478\n",
      "  498 479 464 464 452 463 481 451 461 476 477 463 456 476 451 467 461 451\n",
      "  472 452 451 475 464 478 451 481 498 461 474 478 452 461 451 481 456 475\n",
      "  479 507 481 461 481 451 462 458 451 461 456 464 459 477 498 461 478 451\n",
      "  451 463 454 472 451 459 478 476 481 464 479 459 446 451 462 458 461 479\n",
      "  456 476 479 451 459 457 463 451 463 481 454 462 451 451 462 459 461 479\n",
      "  477 451 481 477 452 451 475 102]\n",
      " [101 464 478 451 477 481 462 456 475 479 507 481 461 481 476 481 474 477\n",
      "  459 119 452 478 507 472 454 478 498 502 479 478 464 507 461 451 477 446\n",
      "  477 481 476 481 451 462 476 479 451 461 459 456 459 481 459 452 481 476\n",
      "  451 461 481 451 463 476 451 457 454 476 451 475 451 452 461 451 455 461\n",
      "  451 463 454 472 451 459 478 451 462 452 451 475 464 478 451 481 476 457\n",
      "  454 479 481 451 475 481 451 472 476 465 477 479 469 481 451 481 456 451\n",
      "  459 464 459 478 451 463 454 119 108 476 463 469 479 459 464 456 451 469\n",
      "  481 467 452 451 467 452 451 481 481 108 459 477 481 451 481 451 459 479\n",
      "  475 472 452 479 461 477 459 477 481 451 481 451 454 472 451 473 451 454\n",
      "  461 479 481 451 481 481 451 463 454 119 452 461 451 481 451 481 477 474\n",
      "  478 452 454 479 451 477 481 476 451 455 451 461 462 481 452 451 481 451\n",
      "  481 477 478 477 461 476 477 459 461 451 459 461 474 474 477 481 476 479\n",
      "  451 462 451 477 478 451 475 460 454 452 452 461 481 476 446 452 451 481\n",
      "  459 460 478 477 458 479 459 461 451 498 460 481 461 451 481 499 481 462\n",
      "  478 451 481 470 481 461 469 451 459 481 479 464 481 479 478 478 451 481\n",
      "  477 451 476 451 475 479 472 474 477 481 476 119 452 479 461 477 478 477\n",
      "  461 476 477 459 481 451 463 454 474 478 459 477 481 451 481 479 451 473\n",
      "  469 481 454 461 451 452 451 454 458 481 475 479 461 479 481 451 498 481\n",
      "  479 477 459 476 481 462 477 459 119 477 473 464 478 451 481 458 451 465\n",
      "  451 479 446 454 451 455 481 461 481 451 462 464 474 475 478 451 481 473\n",
      "  459 481 476 481 446 463 477 454 481 479 479 451 474 477 464 481 452 478\n",
      "  451 481 477 498 459 481 459 478 478 451 463 454 119 451 459 479 475 472\n",
      "  452 479 461 477 446 467 461 451 457 446 474 451 461 481 474 451 454 479\n",
      "  461 481 463 454 446 454 465 479 481 461 463 451 462 479 477 473 451 464\n",
      "  459 461 122 130 124 121 459 461 464 478 461 452 479 459 456 479 481 463\n",
      "  451 462 456 476 478 479 461 481 499 474 452 478 459 477 481 451 451 476\n",
      "  459 119 459 461 451 479 475 481 477 463 451 475 478 451 481 462 477 459\n",
      "  507 481 451 464 452 478 459 475 481 475 479 466 469 481 454 451 473 454\n",
      "  465 451 459 481 476 456 452 102]\n",
      " [101 479 461 452 478 454 461 474 458 451 477 479 451 459 478 451 464 464\n",
      "  459 119 498 459 461 464 474 451 461 507 461 461 451 478 451 478 477 452\n",
      "  479 459 479 454 451 476 481 477 476 469 451 464 462 477 459 507 481 452\n",
      "  461 451 481 464 463 458 454 119 452 479 461 477 452 478 451 454 472 451\n",
      "  473 481 474 481 451 462 451 473 479 451 476 464 452 478 464 478 461 498\n",
      "  461 451 507 476 478 451 456 461 454 474 461 459 119 459 461 122 130 125\n",
      "  130 452 478 454 457 465 481 475 459 461 461 464 454 478 478 477 461 478\n",
      "  451 481 462 481 452 451 459 461 459 451 477 464 507 451 478 499 451 461\n",
      "  475 462 498 461 451 507 476 464 470 479 475 464 459 119 451 455 451 461\n",
      "  452 479 461 477 451 462 451 481 477 463 451 475 452 454 459 461 481 456\n",
      "  459 461 477 464 461 481 451 454 476 454 469 459 459 481 459 461 474 464\n",
      "  479 461 464 452 478 499 451 498 476 481 461 463 459 479 459 461 122 130\n",
      "  128 125 451 462 451 479 452 478 469 477 479 451 477 183 452 478 454 461\n",
      "  481 477 474 451 461 481 474 451 454 479 461 481 463 454 463 451 475 499\n",
      "  474 196 477 451 476 476 481 452 461 477 459 119 451 462 452 479 461 477\n",
      "  452 478 469 477 479 451 477 457 451 474 476 473 475 476 461 479 467 477\n",
      "  462 463 481 451 478 481 451 459 476 481 474 477 477 459 119 464 451 481\n",
      "  459 451 481 477 464 478 461 454 452 478 458 451 467 461 463 461 481 474\n",
      "  451 461 478 451 481 476 464 478 479 461 464 452 451 464 459 474 478 452\n",
      "  451 469 477 479 451 477 452 479 461 479 477 479 507 461 451 472 481 114\n",
      "  454 475 472 481 473 481 467 477 462 451 476 481 462 451 462 451 463 476\n",
      "  479 464 481 479 478 474 451 461 464 113 474 478 452 451 454 474 477 481\n",
      "  474 507 461 451 479 479 461 451 477 456 451 476 476 481 464 459 119 454\n",
      "  474 477 481 474 463 458 454 507 461 451 479 479 461 452 461 451 481 452\n",
      "  479 461 477 459 477 481 451 481 458 451 465 481 461 451 452 478 479 456\n",
      "  479 459 451 479 461 459 454 451 459 461 451 477 451 454 472 451 473 451\n",
      "  454 454 451 461 481 458 481 446 464 458 465 481 454 478 451 481 451 459\n",
      "  452 481 479 473 478 461 476 451 477 451 477 451 472 463 451 477 478 451\n",
      "  481 452 478 507 479 477 478 102]\n",
      " [101 451 481 459 481 507 461 468 478 479 461 474 477 477 459 119 451 459\n",
      "  479 475 472 452 479 461 477 459 461 469 481 477 457 451 475 477 479 481\n",
      "  463 477 459 478 479 451 477 481 476 451 454 479 461 477 481 462 478 463\n",
      "  454 479 452 461 451 481 452 461 477 451 476 478 454 475 479 481 462 481\n",
      "  479 477 481 183 464 452 452 478 458 481 461 452 499 478 478 451 196 452\n",
      "  461 477 451 476 478 476 481 463 451 462 459 119 454 465 479 481 461 463\n",
      "  451 462 481 452 481 464 451 462 123 124 121 456 475 459 474 454 451 452\n",
      "  479 454 469 459 451 459 452 481 464 476 451 461 481 467 461 451 457 481\n",
      "  461 479 481 456 475 459 477 464 451 477 459 478 477 459 478 472 469 451\n",
      "  475 481 454 476 463 454 476 461 479 470 461 481 452 452 479 461 477 459\n",
      "  451 461 459 119 452 479 461 477 459 461 451 455 451 461 464 463 477 454\n",
      "  478 451 479 459 463 454 451 479 461 459 478 451 481 477 473 451 464 481\n",
      "  461 451 452 461 451 481 458 475 473 451 455 451 461 481 452 459 481 469\n",
      "  479 477 479 459 463 454 476 451 481 478 474 451 461 464 473 461 451 461\n",
      "  476 481 459 478 459 479 499 481 462 478 451 481 481 461 451 452 478 454\n",
      "  465 479 481 461 476 481 474 464 459 474 478 454 477 478 451 459 461 457\n",
      "  481 467 478 454 458 481 475 451 479 456 451 461 481 451 463 454 119 452\n",
      "  451 462 477 476 451 481 481 454 458 481 475 481 479 459 461 452 469 466\n",
      "  481 476 479 451 473 469 459 478 464 454 477 451 474 452 479 461 477 446\n",
      "  451 462 477 481 451 454 464 481 467 451 477 481 451 459 476 478 451 458\n",
      "  452 461 476 481 459 478 459 479 464 461 451 461 454 478 451 481 451 477\n",
      "  463 451 477 481 459 461 473 451 475 452 477 476 451 459 478 451 458 479\n",
      "  459 461 451 477 464 451 477 476 481 459 478 477 459 119 459 461 459 477\n",
      "  481 451 481 452 479 461 477 478 476 478 499 481 462 476 481 454 479 451\n",
      "  477 459 451 454 472 451 473 452 481 472 454 459 131 451 459 476 478 451\n",
      "  479 102   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]]\n",
      "(6, 512) {<class 'int'>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "labels_list\n",
      " [[-100    1    0    0    0    0    1    0    0    0    0    1    0    0\n",
      "     0    1    0    1    0    0    0    0    0    1    0    0    0    1\n",
      "     0    0    0    1    0    0    1    1    1    0    0    0    0    0\n",
      "     0    1    1    0    0    0    0    0    0    0    1    0    0    0\n",
      "     0    0    1    0    0    0    0    0    1    1    0    0    0    0\n",
      "     2    0    0    0    0    0    1    0    2    0    0    0    0    1\n",
      "     0    1    0    0    0    1    0    0    1    0    0    0    0    1\n",
      "     0    0    0    0    1    0    0    0    1    0    1    0    0    0\n",
      "     0    0    1    0    0    0    1    0    1    0    0    0    1    0\n",
      "     0    0    0    0    1    0    1    0    0    0    0    0    1    0\n",
      "     0    0    0    1    0    0    1    0    1    0    1    0    0    0\n",
      "     0    1    0    0    0    0    1    0    1    0    0    0    1    0\n",
      "     0    0    0    1    0    0    0    0    2    0    1    0    0    1\n",
      "     0    0    2    0    0    0    1    0    0    0    1    0    0    0\n",
      "     0    2    0    0    0    0    0    1    0    0    0    1    0    0\n",
      "     1    0    0    0    0    2    0    1    0    1    0    0    0    0\n",
      "     0    1    0    0    0    0    1    0    0    0    0    1    1    0\n",
      "     0    1    0    1    0    0    0    1    0    0    0    0    0    1\n",
      "     0    0    0    2    0    1    1    0    0    0    0    0    1    0\n",
      "     1    0    0    0    1    0    1    0    0    0    0    1    0    0\n",
      "     0    0    1    0    0    1    0    2    0    0    0    0    0    0\n",
      "     0    1    0    0    0    1    0    1    0    0    1    0    0    1\n",
      "     0    0    0    0    0    1    0    0    1    0    0    0    0    0\n",
      "     1    0    0    0    1    0    0    0    1    0    2    0    1    0\n",
      "     1    0    0    1    0    0    0    1    0    1    0    0    1    0\n",
      "     0    0    0    1    0    0    0    0    2    0    1    0    1    0\n",
      "     0    0    1    0    0    0    1    0    0    0    1    0    1    0\n",
      "     0    0    0    1    0    0    0    0    1    0    2    0    0    2\n",
      "     0    0    0    1    0    0    1    0    0    0    0    2    0    0\n",
      "     0    0    0    1    0    0    0    1    0    0    0    0    0    1\n",
      "     0    0    0    1    0    1    0    0    0    1    0    0    0    0\n",
      "     0    1    0    0    0    0    1    0    0    0    2    0    0    0\n",
      "     1    0    0    0    1    0    0    0    0    1    0    0    0    0\n",
      "     2    0    1    0    1    0    0    0    2    0    0    1    0    1\n",
      "     0    1    0    0    0    0    0    1    0    0    0    1    0    0\n",
      "     0    1    0    1    0    0    0    0    1    0    0    0    0    1\n",
      "     0    0    1    0    0    0    1 -100]\n",
      " [-100    0    0    1    0    1    0    0    1    0    0    0    0    0\n",
      "     1    0    0    0    0    1    0    0    0    0    0    0    0    1\n",
      "     0    0    1    0    0    0    0    0    1    0    0    0    1    0\n",
      "     1    0    1    0    0    0    0    0    0    1    0    2    0    0\n",
      "     0    1    0    0    0    0    0    1    0    0    1    0    0    0\n",
      "     0    0    0    0    0    1    0    0    0    1    0    0    1    0\n",
      "     0    1    0    0    0    0    0    1    0    0    0    0    2    0\n",
      "     0    1    0    0    0    1    0    2    0    0    0    0    1    0\n",
      "     0    0    1    0    0    0    0    1    0    0    1    1    0    0\n",
      "     0    0    1    0    0    0    1    0    0    0    0    1    0    0\n",
      "     0    0    1    0    0    0    1    0    0    0    0    0    0    1\n",
      "     0    0    0    1    0    0    0    0    1    0    0    0    0    1\n",
      "     0    0    1    0    0    0    0    1    0    1    0    0    0    0\n",
      "     1    0    0    0    1    0    0    0    1    0    0    0    1    0\n",
      "     1    0    0    1    0    0    0    0    1    0    0    0    1    1\n",
      "     0    0    0    0    1    0    0    0    1    0    0    0    0    1\n",
      "     0    0    1    1    0    0    0    1    0    0    0    0    1    0\n",
      "     0    1    0    2    0    0    0    0    1    0    1    0    0    0\n",
      "     0    0    1    0    0    0    0    0    1    0    0    0    0    1\n",
      "     1    0    0    0    0    1    0    0    0    1    0    0    1    0\n",
      "     0    0    1    0    0    0    0    1    0    0    0    1    0    1\n",
      "     0    0    0    0    1    0    0    0    1    0    0    0    0    1\n",
      "     0    0    1    0    1    0    0    0    0    1    0    0    0    0\n",
      "     0    0    1    0    0    0    0    1    0    0    0    0    1    0\n",
      "     0    0    0    0    0    1    0    0    0    0    1    0    0    0\n",
      "     1    0    0    0    1    0    0    1    0    0    0    0    0    0\n",
      "     0    1    0    0    0    1    0    0    0    0    0    1    0    0\n",
      "     0    0    1    0    0    0    1    0    0    0    0    1    0    1\n",
      "     0    0    0    1    0    0    1    1    0    0    1    0    0    1\n",
      "     0    0    0    1    0    1    0    0    0    0    1    0    0    0\n",
      "     0    1    0    2    0    0    0    1    0    0    0    1    0    0\n",
      "     1    0    0    0    0    0    0    1    0    2    0    0    0    0\n",
      "     0    1    0    1    0    0    0    1    0    0    1    0    0    1\n",
      "     0    0    1    0    0    0    0    1    0    0    0    1    0    1\n",
      "     0    0    0    0    1    0    0    0    0    1    0    1    0    1\n",
      "     0    0    0    0    0    1    0    0    0    1    0    0    0    0\n",
      "     1    0    1    0    0    1    0 -100]\n",
      " [-100    2    0    0    1    1    0    1    0    0    1    0    0    1\n",
      "     0    2    0    0    0    0    1    0    0    0    0    0    1    0\n",
      "     1    0    0    0    0    0    1    0    0    0    0    0    1    0\n",
      "     1    0    0    0    0    1    0    0    1    0    0    0    1    0\n",
      "     0    0    0    1    0    0    0    1    0    0    0    0    0    0\n",
      "     1    0    0    0    0    1    0    1    0    0    1    0    0    0\n",
      "     1    0    1    0    0    0    2    0    0    1    0    1    0    0\n",
      "     0    0    0    0    1    0    0    0    0    1    1    0    0    0\n",
      "     0    1    1    0    0    0    1    0    0    0    0    1    0    0\n",
      "     1    1    0    0    0    0    0    0    1    0    1    0    0    0\n",
      "     2    0    0    1    0    1    0    0    0    1    0    0    0    0\n",
      "     0    0    1    0    0    0    0    1    0    1    0    0    0    1\n",
      "     0    0    0    0    0    0    1    0    1    0    0    0    2    0\n",
      "     0    1    0    0    0    0    2    0    0    1    0    1    0    0\n",
      "     0    0    1    0    0    0    0    0    1    0    0    0    1    0\n",
      "     1    0    0    0    0    1    0    0    0    0    0    0    1    0\n",
      "     1    0    0    0    0    1    0    0    0    0    0    0    0    0\n",
      "     1    0    0    1    0    1    0    0    0    0    0    0    0    0\n",
      "     0    0    0    1    0    0    0    0    0    1    0    2    0    0\n",
      "     0    0    0    1    0    0    0    1    0    0    0    0    0    2\n",
      "     0    0    1    0    0    0    0    1    0    1    0    0    0    0\n",
      "     1    0    0    0    0    0    1    0    0    0    1    1    0    1\n",
      "     1    0    0    0    0    1    0    0    0    1    0    0    0    0\n",
      "     0    2    0    0    1    0    1    0    0    0    1    0    0    0\n",
      "     0    0    0    0    0    1    0    1    0    0    0    0    1    1\n",
      "     0    0    0    0    0    1    0    1    0    0    1    0    0    0\n",
      "     0    1    0    1    0    0    0    0    0    0    1    0    0    0\n",
      "     0    1    0    0    0    0    1    0    0    0    0    0    1    1\n",
      "     0    1    0    1    0    0    1    0    0    0    0    1    0    0\n",
      "     0    0    2    0    0    1    0    1    0    0    0    1    0    0\n",
      "     0    0    1    0    0    0    0    1    0    0    0    0    1    0\n",
      "     0    0    0    0    0    1    0    1    0    1    0    0    0    1\n",
      "     0    0    0    0    0    0    1    0    1    0    0    0    1    0\n",
      "     0    1    0    0    0    1    0    0    0    0    0    0    1    0\n",
      "     2    0    0    0    1    0    1    0    0    0    1    0    0    0\n",
      "     1    0    0    0    0    0    2    0    1    0    1    0    0    0\n",
      "     1    0    0    1    0    0    0 -100]\n",
      " [-100    2    0    1    0    0    1    0    0    0    0    0    0    1\n",
      "     0    2    0    0    0    1    0    1    0    0    0    1    0    0\n",
      "     0    0    0    0    0    0    0    1    0    0    0    1    0    1\n",
      "     0    0    0    0    1    0    0    0    1    0    0    0    0    0\n",
      "     1    0    0    1    0    0    0    0    0    0    1    0    1    0\n",
      "     0    1    0    0    0    0    0    0    1    0    1    0    0    0\n",
      "     0    0    0    1    0    0    0    0    1    0    0    0    0    1\n",
      "     0    0    0    0    0    1    0    0    0    0    1    0    0    1\n",
      "     0    0    0    1    1    0    0    0    0    1    0    0    0    0\n",
      "     1    0    0    0    0    0    0    0    1    1    0    0    0    0\n",
      "     1    0    0    0    0    1    0    0    0    1    0    0    0    0\n",
      "     1    0    0    0    0    0    0    1    0    0    0    0    0    1\n",
      "     0    0    0    1    0    0    0    1    0    0    1    0    1    0\n",
      "     0    0    0    0    0    1    0    0    0    1    0    0    0    0\n",
      "     1    0    0    1    0    0    0    0    0    1    0    1    0    0\n",
      "     1    0    0    0    1    1    0    1    0    0    0    1    0    0\n",
      "     1    0    0    0    0    0    1    0    0    0    1    0    0    1\n",
      "     0    0    1    0    1    0    0    0    0    0    1    0    0    0\n",
      "     0    0    1    0    0    0    0    0    0    1    1    0    0    0\n",
      "     2    0    0    1    0    0    0    0    0    0    1    0    0    0\n",
      "     0    1    0    0    0    1    0    0    0    0    0    0    1    0\n",
      "     0    1    0    1    0    0    0    0    1    0    0    0    0    0\n",
      "     1    0    1    0    1    0    0    0    1    1    0    0    0    1\n",
      "     0    0    0    0    1    0    2    0    0    0    1    0    0    0\n",
      "     0    0    1    0    0    1    0    0    1    0    0    0    0    0\n",
      "     1    0    1    0    0    0    0    0    1    0    0    0    0    0\n",
      "     1    0    0    0    1    1    0    0    0    0    0    1    0    1\n",
      "     0    0    1    0    0    0    0    2    0    0    0    0    1    0\n",
      "     0    0    0    1    0    0    0    0    1    0    0    0    0    1\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    1    0\n",
      "     0    0    0    0    0    0    1    1    0    0    0    1    0    1\n",
      "     0    0    0    1    0    1    0    0    1    0    0    0    0    0\n",
      "     0    1    0    1    0    0    0    0    0    1    0    1    0    1\n",
      "     0    0    0    1    0    0    0    1    0    1    0    0    0    0\n",
      "     1    0    0    0    0    0    1    0    0    0    0    2    0    1\n",
      "     0    1    0    0    0    1    0    0    0    0    1    0    0    0\n",
      "     0    0    0    1    0    0    0 -100]\n",
      " [-100    0    1    0    1    0    0    1    0    0    0    0    0    0\n",
      "     2    0    1    0    0    1    0    0    0    1    0    0    0    0\n",
      "     1    0    0    2    0    0    1    0    0    1    1    0    0    0\n",
      "     0    1    0    0    0    1    0    0    0    0    1    0    0    0\n",
      "     0    1    0    0    0    1    0    0    0    1    0    1    0    0\n",
      "     0    0    1    0    0    1    0    1    0    0    0    0    0    1\n",
      "     0    1    0    0    1    0    0    0    1    0    0    0    0    0\n",
      "     1    0    0    0    1    0    1    0    0    0    1    0    1    0\n",
      "     0    0    0    1    0    1    0    0    0    1    0    0    0    0\n",
      "     0    1    0    0    0    1    0    1    0    0    0    0    0    0\n",
      "     1    0    0    0    0    1    0    0    0    1    0    0    0    0\n",
      "     1    0    0    1    0    0    0    1    0    0    0    1    0    1\n",
      "     0    0    1    0    0    1    0    0    0    0    0    1    0    1\n",
      "     0    0    0    0    0    1    0    0    0    0    0    1    0    1\n",
      "     0    0    0    0    1    0    1    0    0    1    0    2    0    0\n",
      "     1    1    0    1    0    0    0    1    0    1    0    1    0    1\n",
      "     0    0    0    0    1    0    0    0    0    0    0    1    0    0\n",
      "     0    0    0    0    0    0    0    0    0    1    0    0    1    0\n",
      "     0    1    0    0    1    0    2    0    0    0    0    1    0    1\n",
      "     0    0    0    1    0    1    0    0    0    0    1    0    0    0\n",
      "     1    0    0    0    0    1    0    0    1    0    0    0    1    0\n",
      "     0    1    0    2    0    0    0    0    1    0    0    0    1    0\n",
      "     0    1    0    0    0    1    0    1    0    0    0    1    0    0\n",
      "     1    0    0    0    0    0    1    0    0    0    0    0    1    0\n",
      "     0    0    1    0    1    0    1    0    0    0    0    1    0    0\n",
      "     0    0    0    0    0    0    0    0    0    1    0    0    0    0\n",
      "     0    1    0    0    0    0    0    0    1    0    1    0    0    1\n",
      "     1    0    0    0    1    0    0    0    1    0    0    1    0    1\n",
      "     0    0    0    0    1    0    0    0    0    0    1    0    0    0\n",
      "     0    1    0    2    0    0    1    0    0    0    0    1    0    0\n",
      "     1    0    0    0    0    0    1    0    0    0    1    0    0    0\n",
      "     1    0    0    0    0    1    0    0    0    1    0    1    0    1\n",
      "     0    0    0    1    0    0    0    1    0    1    0    1    0    1\n",
      "     0    0    0    0    0    0    1    0    0    0    0    0    0    1\n",
      "     0    0    0    0    0    0    0    1    0    0    0    1    1    0\n",
      "     0    0    0    0    0    0    1    0    0    0    0    0    2    0\n",
      "     1    0    1    0    0    0    2 -100]\n",
      " [-100    0    1    0    0    0    1    0    0    0    1    0    0    0\n",
      "     0    1    0    0    0    0    1    0    0    0    1    0    0    0\n",
      "     0    2    0    0    1    0    0    0    0    0    0    1    1    0\n",
      "     0    0    0    0    0    0    1    0    0    1    0    0    1    1\n",
      "     0    0    0    1    0    0    0    0    0    1    0    0    0    0\n",
      "     0    0    0    0    1    0    0    2    0    2    0    0    1    0\n",
      "     0    2    0    0    1    0    0    0    0    0    1    0    2    0\n",
      "     0    0    0    1    0    0    0    0    0    0    0    0    1    0\n",
      "     0    1    0    1    0    0    1    0    0    1    0    0    0    1\n",
      "     1    0    0    0    0    1    0    0    0    0    0    0    1    0\n",
      "     0    0    0    1    0    0    1    0    0    1    0    0    0    2\n",
      "     0    0    0    0    1    0    0    0    0    0    1    0    0    0\n",
      "     0    1    1    0    0    0    1    0    0    0    1    0    0    0\n",
      "     0    1    0    0    0    1    0    1    0    0    0    0    1    0\n",
      "     0    2    0    1    1    0    0    0    0    0    0    0    0    0\n",
      "     1    0    0    0    0    1    0    1    0    0    0    1    0    0\n",
      "     1    0    0    0    0    1    0    0    0    1    1    0    1    0\n",
      "     0    0    0    0    0    1    0    0    0    1    0    0    0    1\n",
      "     0    2    0    0    1    1    0    0    0    0    0    0    1    0\n",
      "     1    0    1    0    0    0    0    1    0    2    0    0    1    0\n",
      "     1    0    0    0    1    0    1    0    0    0    1    0    0    0\n",
      "     1    0    1    0    0    0    1    0    0    0    1    0    0    0\n",
      "     0    0    0    0    1    0    0    0    0    1    1    0    1    0\n",
      "     0    0    1    0    0    0    0    1    0    0    0    0    0    0\n",
      "     1    0    0    0    0    1    0    1    0    0    0    1    0    0\n",
      "     0    0    0    1    0    0    0    0    1    0    0    1    0    2\n",
      "     0    0    1    1    0    0    0    0    2    0    0    1    0    0\n",
      "     0    0    0    1    0    1    0    0    0    1    0    0    0    0\n",
      "     0    1    0    0    1    0    1    0    0    0    1    0    2    0\n",
      "     0    0    0    1    0    1    0    0    0    0    1    0    0    0\n",
      "     1    0    0    1    0    0    1    0    2    0    0    0    0    1\n",
      "     0    0    0    0    1    0    0    0    0    0    1    0    0    0\n",
      "     0    1    1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100]]\n",
      "(6, 512) {<class 'int'>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "attention_mask_list\n",
      " [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0]]\n",
      "(6, 512) {<class 'int'>}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "atmask=np.array(attention_mask_list).shape\n",
    "inpid=np.array(input_ids_list).shape\n",
    "labls=np.array(labels_list).shape\n",
    "\n",
    "print(atmask,inpid,labls)\n",
    "def count_unique_types(two_d_list):\n",
    "    unique_types = set()\n",
    "    for sublist in two_d_list:\n",
    "        for item in sublist:\n",
    "            unique_types.add(type(item))\n",
    "    return unique_types\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\n",
    "\n",
    "print('input_ids_list\\n',np.array(input_ids_list))\n",
    "print(inpid,count_unique_types(input_ids_list))  # Output: 4\n",
    "\n",
    "\n",
    "print('\\n\\n\\n\\nlabels_list\\n',np.array(labels_list))\n",
    "print(labls,count_unique_types(labels_list))  # Output: 4\n",
    "\n",
    "print('\\n\\n\\n\\nattention_mask_list\\n',np.array(attention_mask_list))\n",
    "print(atmask,count_unique_types(attention_mask_list))  # Output: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce77c17f921abec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T09:59:29.939902228Z",
     "start_time": "2023-10-26T09:59:29.899491341Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# np.array(labels_list)[:]\n",
    "# tokenizer.decode(input_ids_list[0])\n",
    "\n",
    "\n",
    "# object from the tokenized inputs and labels\n",
    "dataset = Dataset.from_dict({\"input_ids\": input_ids_list, \"labels\": labels_list})\n",
    "# train_dataset = torch.utils.data.Dataset()\n",
    "# train_dataset.encodings = input_ids_list\n",
    "# train_dataset.labels = labels_list\n",
    "# train_dataset.attention_mask = attention_mask_list\n",
    "\n",
    "# Initializing a data collator for token classification with the pre-trained tokenizer\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "639516db106c9ad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T09:59:32.946358601Z",
     "start_time": "2023-10-26T09:59:32.934925629Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 6\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare your data\n",
    "\n",
    "# Create a Dataset object\n",
    "data = Dataset.from_dict({\"input_ids\": input_ids_list, \"labels\": labels_list,})\n",
    "data\n",
    "# \"attention_mask\":attention_mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed7cce0a20e01698",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T10:00:55.776905622Z",
     "start_time": "2023-10-26T10:00:54.521500047Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(pretrained_model,num_labels=3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6fc7780c16a7bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T11:37:49.363629090Z",
     "start_time": "2023-10-26T11:37:49.299791340Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Setting up training arguments for the Trainer class\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    num_train_epochs=200,\n",
    "    # max_steps=200,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    # warmup_steps=500,2\n",
    "    logging_dir=model_dir+'/logs',\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "# Initializing a Trainer instance with the model, training arguments, dataset, tokenizer, and data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66befd791cea6e07",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-26T11:37:50.845143056Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matin/Desktop/Masters/venv/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/200 : < :, Epoch 1/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Training the model on the dataset\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m trainer\u001B[38;5;241m.\u001B[39msave_model(model_dir \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/transformers/trainer.py:1539\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1534\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[1;32m   1536\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[1;32m   1537\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[1;32m   1538\u001B[0m )\n\u001B[0;32m-> 1539\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1540\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1541\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1542\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1544\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/transformers/trainer.py:1809\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1806\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m   1808\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[0;32m-> 1809\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1811\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1812\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   1813\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[1;32m   1814\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   1815\u001B[0m ):\n\u001B[1;32m   1816\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   1817\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/transformers/trainer.py:2654\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2651\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   2653\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 2654\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2656\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   2657\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/transformers/trainer.py:2679\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2677\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2678\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2679\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2680\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   2681\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   2682\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1756\u001B[0m, in \u001B[0;36mBertForTokenClassification.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1751\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1752\u001B[0m \u001B[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1754\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1756\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1757\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1758\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1759\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1760\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1761\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1762\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1763\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1764\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1765\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1766\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1768\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1770\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(sequence_output)\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1022\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1013\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m   1015\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m   1016\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   1017\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1020\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m   1021\u001B[0m )\n\u001B[0;32m-> 1022\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1023\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1025\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1026\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1027\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1028\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1029\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1030\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1031\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1032\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1033\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1034\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1035\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:612\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    603\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    604\u001B[0m         create_custom_forward(layer_module),\n\u001B[1;32m    605\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    609\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    610\u001B[0m     )\n\u001B[1;32m    611\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 612\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    614\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    615\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    616\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    617\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    618\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    619\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    620\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:497\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    485\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    486\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    487\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    494\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    495\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    496\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 497\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    504\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    506\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:427\u001B[0m, in \u001B[0;36mBertAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    418\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    419\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    425\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    426\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 427\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    435\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    436\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    437\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:359\u001B[0m, in \u001B[0;36mBertSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    355\u001B[0m attention_probs \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39msoftmax(attention_scores, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    357\u001B[0m \u001B[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001B[39;00m\n\u001B[1;32m    358\u001B[0m \u001B[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001B[39;00m\n\u001B[0;32m--> 359\u001B[0m attention_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_probs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;66;03m# Mask heads if we want to\u001B[39;00m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m head_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/torch/nn/modules/dropout.py:59\u001B[0m, in \u001B[0;36mDropout.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Masters/venv/lib/python3.9/site-packages/torch/nn/functional.py:1252\u001B[0m, in \u001B[0;36mdropout\u001B[0;34m(input, p, training, inplace)\u001B[0m\n\u001B[1;32m   1250\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m p \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m p \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1.0\u001B[39m:\n\u001B[1;32m   1251\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdropout probability has to be between 0 and 1, \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(p))\n\u001B[0;32m-> 1252\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _VF\u001B[38;5;241m.\u001B[39mdropout_(\u001B[38;5;28minput\u001B[39m, p, training) \u001B[38;5;28;01mif\u001B[39;00m inplace \u001B[38;5;28;01melse\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model on the dataset\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "trainer.save_model(model_dir + \"model/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a4541f5d0d143",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-15T16:47:17.540607610Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir + \"model/\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
